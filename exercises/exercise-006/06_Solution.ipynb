{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY313,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b2291",
   "metadata": {},
   "source": [
    "# Solution to exercise set 6: Classification\n",
    "\n",
    "The main goals of this exercise are to create classifiers and calculate and interpret some performance metrics that can be used to assess the classifiers.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Create classification models.\n",
    "- Create and interpret the confusion matrix and use it to evaluate classifier performance.\n",
    "- Visualise how a decision tree is making its classification.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [6.1(b)](#6.1(b)), [6.1(c)](#6.1(c)), [6.1(d)](#6.1(d)), and [6.1(e)](#6.1(e)): To show that you can create a decision tree, plot the confusion matrix and visualise the decision tree itself, and compare classifiers using the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b4748",
   "metadata": {},
   "source": [
    "## Exercise 6.1 Penguins\n",
    "\n",
    "In this exercise, we will have a look at penguins! We will attempt to figure out the species of penguins based\n",
    "on their bill length, bill depth, flipper length, and body mass.\n",
    "The data is from a paper by \n",
    "[Gorman, Williams, and Fraser](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081)\n",
    "and can also be found in the R package [palmerpenguins](https://github.com/allisonhorst/palmerpenguins).\n",
    "Here, we will use a version of the data set [penguins.csv](penguins.csv) where missing\n",
    "values have been removed and we only have two species of penguins: [Adelie](https://en.wikipedia.org/wiki/Ad%C3%A9lie_penguin) and [Chinstrap](https://en.wikipedia.org/wiki/Chinstrap_penguin).\n",
    "\n",
    "The image below shows the three islands where these penguins can be found (click the image to make it larger): \n",
    "| <a href=\"penguins.png\"><img src=\"penguins2.png\" width=\"50%\"></a>           |\n",
    "|:-:|\n",
    "| **Fig. 1** *Location of islands and images of the penguin species.*    |\n",
    "\n",
    "\n",
    "You will find seven columns in the [penguins.csv](./Data/penguins.csv) data file. Each row is a measurement for\n",
    "a single penguin for the seven variables found in the columns:\n",
    "\n",
    "\n",
    "| Column            |  Description                                                        |\n",
    "|:------------------|--------------------------------------------------------------------:|\n",
    "| species           | The species (Chinstrap or Adelie)                                   |\n",
    "| island            | The island where the observation was made (Dream/Torgersen/Biscoe)  |\n",
    "| bill_length_mm    | (See the illustration below) (measured in mm)                       |\n",
    "| bill_depth_mm     | (See the illustration below) (measured in mm)                       |\n",
    "| flipper_length_mm | (See the illustration below) (measured in mm)                       |\n",
    "| body_mass_g       | The weight of the penguin (in grams)                                |\n",
    "| sex               | Female/Male                                                         |\n",
    "\n",
    "\n",
    "| <img src=\"bill.png\" width=\"50%\">                                   |\n",
    "|:-:|\n",
    "| **Fig. 2** *Illustration of bill length, bill depth, and flipper length. (The foot is not used in this data set.)*    |\n",
    "\n",
    "The data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f161cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"penguins.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dad47d",
   "metadata": {},
   "source": [
    "### 6.1(a)\n",
    "\n",
    "**Task: Investigate (by creating figures) if the variables `bill_length_mm`, `bill_depth_mm`,\n",
    "`flipper_length_mm`, and `body_mass_g` can be used to separate the different\n",
    "species.**\n",
    "\n",
    "**Hint:** Several plots can be used to get an overview of the data. For instance, the [scatter plot matrix](https://seaborn.pydata.org/examples/scatterplot_matrix.html), [jointplot](https://seaborn.pydata.org/tutorial/introduction.html#multivariate-views-on-complex-datasets)\n",
    "from seaborn, or a [boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html). Here is one example of how to create the scatter plot matrix:\n",
    "```python\n",
    "# To create a scatter plot matrix with seaborn:\n",
    "grid = sns.pairplot(\n",
    "    data,\n",
    "    corner=True,\n",
    "    hue=\"species\",  # Hue is used to select a column from data to use for colouring\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435cfba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid = sns.pairplot(\n",
    "    data,\n",
    "    corner=True,\n",
    "    hue=\"species\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80184e",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(a): Are there any promising variables that could separate the species?\n",
    "Yes, `bill_length_mm` shows promise for separating species. The scatter plot suggests that different species tend to have distinct ranges of bill lengths, with Chinstrap penguins exhibiting larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab6e77",
   "metadata": {},
   "source": [
    "### 6.1(b)\n",
    "\n",
    "**Task: Create a training set and a test set to use to classify the penguin species. What is the fraction of Adelie penguins in the original, test, and training data?**\n",
    "\n",
    "**Hint:** With scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), splitting the data can be done with\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    stratify=y\n",
    ")\n",
    "```\n",
    "In the example above, we use stratification for the y-values, this is to **ensure that each split** (training and testing) **contains approximately the same proportion of samples from each class as the original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6919d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "]\n",
    "\n",
    "X = data[variables]\n",
    "y = data[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd17c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd256910",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Adelie\"\n",
    "\n",
    "for yi, label in ((y, \"original\"), (y_test, \"test\"), (y_train, \"train\")):\n",
    "    number = sum(yi == species)\n",
    "    total = len(yi)\n",
    "    frac = number / total\n",
    "    print(f\"Data set {label}\")\n",
    "    print(f\"\\tNumber of {species} penguins: {number}\")\n",
    "    print(f\"\\tTotal number of penguins: {total}\")\n",
    "    print(f\"\\tFraction of {species} penguins: {frac:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a567",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(b): What is the fraction of Adelie penguins?\n",
    "The fraction of Aelie penguins is around 0.68 in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f980a",
   "metadata": {},
   "source": [
    "### 6.1(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify the penguin species. Use two levels for the tree and show the confusion matrix for the training and the test set. Is your classifier making any mistakes?**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. A decision tree can be created using scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the tree. The parameter max_depth selects the number of levels in the tree\n",
    "my_first_tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Train the tree:\n",
    "my_first_tree.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "2. To show the confusion matrix:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    my_first_tree,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    colorbar=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d954897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_tree = DecisionTreeClassifier(max_depth=2)\n",
    "my_first_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10500d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].set_title(\"Train:\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    my_first_tree,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    colorbar=False,\n",
    "    ax=axes[0],\n",
    ")\n",
    "fig.colorbar(axes[0].images[0], ax=axes[0], shrink=0.5)\n",
    "\n",
    "axes[1].set_title(\"Test:\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    my_first_tree, X_test, y_test, colorbar=False, ax=axes[1]\n",
    ")\n",
    "fig.colorbar(axes[1].images[0], ax=axes[1], shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff977844",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(c): Is your classifier making any mistakes?\n",
    "\n",
    "es, the classifier makes a few misclassifications in both the training and test sets. Specifically, for the test set, it misclassifies two Chinstrap penguins as Adelie and two Adelie penguins as Chinstrap. Despite these errors, the classifier demonstrates a high number of correct classifications. We can quantify its performance using accuracy, calculated as the ratio of correct predictions to the total number of predictions. In this case:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{35 + 15}{35 + 15 + 2 + 2} = \\frac{50}{54} \\approx 0.9259\n",
    "\\end{equation}\n",
    "\n",
    "We can also make scikit-learn calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, my_first_tree.predict(X_test))\n",
    "print(f\"Accuracy: {acc:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b6c86",
   "metadata": {},
   "source": [
    "In conclusion, it makes some mistakes, but the accuracy is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842d87d",
   "metadata": {},
   "source": [
    "### 6.1(d)\n",
    "\n",
    "**Task: Visualise your decision tree and use this to describe how the classification is made.**\n",
    "\n",
    "**Hint:** The decision tree can be visualized using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) or [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html),\n",
    "\n",
    "1. Using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html):\n",
    "\n",
    "```python\n",
    "from sklearn import tree\n",
    "\n",
    "variables = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "]\n",
    "\n",
    "tree.plot_tree(\n",
    "    my_first_tree,  # The tree to plot\n",
    "    filled=True,  # Add color to the boxes.\n",
    "    feature_names=variables,  # Get name for variables from the variables list.\n",
    "    class_names=my_first_tree.classes_,  # Get the name of the different classes from the tree.\n",
    ")\n",
    "```\n",
    "\n",
    "2. Alternative: Using [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html):\n",
    "\n",
    "```python\n",
    "from sklearn.tree import export_graphviz  # To create the tree.\n",
    "import graphviz  # To turn the three into a graph, you may need to install this (pip install graphviz).\n",
    "from IPython.display import display  # To show the graph.\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    my_first_tree,  # The tree to plot.\n",
    "    out_file=None,  # Do not write to file.\n",
    "    feature_names=variables,  # Get name for variables from the variables list.\n",
    "    class_names=my_first_tree.classes_,  # Get the name of the different classes from the tree.\n",
    "    rounded=True,  # Show the boxes in the tree with rounded corners.\n",
    "    filled=True,  # Add color to the boxes.\n",
    ")\n",
    "display(graphviz.Source(dot_data))  # Show the tree in a notebook.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz  # To create the tree\n",
    "import graphviz  # To turn the three into a graph, you may need to install this (pip install graphviz).\n",
    "from IPython.display import display  # To show the tree\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    my_first_tree,\n",
    "    out_file=None,\n",
    "    feature_names=variables,\n",
    "    class_names=my_first_tree.classes_,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "display(graphviz.Source(dot_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a17287",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(d): Describe the decision-making process of your classifier.\n",
    "\n",
    "The decision tree classifier uses only the `bill_length_mm` feature to classify the penguins. At the root node, it checks if the `bill_length_mm` is less than or equal to 43.35. If it is true, then the penguin is classified as Adelie. If the `bill_length_mm` is larger than 43.35, then the penguin is classified as a Chinstrap. The tree includes further splits on the `bill_length_mm` feature in the second level, but this does not improve the classification significantly. Therefore, we could probably have used just one level for the decision tree in this case. This is consistent with the observations made in [6.1(a)](#Your-answer-to-question-6.1(a):-Are-there-any-promising-variables-that-could-separate-the-species?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f484ff",
   "metadata": {},
   "source": [
    "### 6.1(e)\n",
    "\n",
    "The figure below compares a decision tree classifier to a k-nearest neighbours classifier (using one neighbour) for the test set.\n",
    "\n",
    "**Task: Use the figure to compare the two classifiers (the left part shows the confusion matrix of the tree classifier applied to the test set, and the right part shows the k-nearest neighbours classifier applied to the same test set). Which one performs best?**\n",
    "\n",
    "![Compare classifiers](comparecls.png)\n",
    "\n",
    "**Note:** Your confusion matrix in [6.1(c)](#6.1(c)) may differ from the one shown here since the splitting into a test and training set is randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408da68",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(e): Which of the two classifiers performs best?\n",
    "The tree classifier performs best as it makes fewer errors of both types. Calculating the accuracy we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy(tree)} = \\frac{35 + 15}{35 + 15 + 2 + 2} = \\frac{50}{54} \\approx 0.9259\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy(k-nearest-neigbours)} = \\frac{32 + 11}{32 + 11 + 5 + 6} = \\frac{43}{54} \\approx 0.7963\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the accuracy of the k-nearest-neigbours classifier is lower than the accuracy of the decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44457c41",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "\n",
    "[Schummer *et al.*](https://doi.org/10.1016/S0378-1119(99)00342-X) used microarray technology to analyze the expression of 1536 genes in ovarian cancer and non-cancer tissues. Their primary objective was to identify differentially expressed genes in ovarian cancer versus non-cancer tissues to discover genes with diagnosis potential.\n",
    "\n",
    "The data file [`ovo.csv`](ovo.csv) contains numerical gene expressions (for 1536 genes) for 54 tissue samples. Each column corresponds to a specific gene, named `X.1`, `X.2`, and so on. Each tissue sample has been classified as non-cancer (`N`) or cancer (`C`) tissue, and these labels can be found in the column `class`. The raw data has been preprocessed by centring each gene expression so that no further preprocessing is needed. The raw data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the data set.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "data_ovo = pd.read_csv(\"ovo.csv\")\n",
    "classes = data_ovo[\"class\"]  # Classification of samples.\n",
    "# Turn the class labels into numbers for numeric methods\n",
    "y_ovo = [1 if i == \"C\" else 0 for i in classes]\n",
    "X_ovo = data_ovo.filter(like=\"X.\", axis=1)  # Gene expressions for samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2574ba0",
   "metadata": {},
   "source": [
    "### 6.2(a)\n",
    "\n",
    "**Task: Explore the raw data. Do you find genes that appear to show significant differences in expression between non-cancer and cancer tissue?**\n",
    "\n",
    "**Hint:** You can, for instance, inspect the raw data by running a principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "comp = range(1, len(pca.explained_variance_ratio_) + 1)\n",
    "ax1.bar(comp, 100 * pca.explained_variance_ratio_)\n",
    "ax2.plot(comp, 100 * np.cumsum(pca.explained_variance_ratio_), marker=\"o\")\n",
    "ax1.set(xlabel=\"PC no.\", ylabel=\"Explained variance (%)\")\n",
    "ax2.set(xlabel=\"Number of PCs used\", ylabel=\"Explained variance (%)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d736ec",
   "metadata": {},
   "source": [
    "The explained variance indicates that a substantial number of principal components are required to capture a significant portion of the variance in the data. However, the scores of the first few principal components show interesting patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 1], hue=\"class\", ax=axes[0]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 2], hue=\"class\", ax=axes[1]\n",
    ")\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "axes[0].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "axes[1].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC3 ({percent[2]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "    ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee7718",
   "metadata": {},
   "source": [
    "The PC1 scores separate the two tissue types. To understand what variables are important for this separation, we inspect the loadings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ec358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "\n",
    "\n",
    "loadings = pca.components_\n",
    "pc1_loadings = loadings[0, :]\n",
    "pc2_loadings = loadings[1, :]\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "ax.set(\n",
    "    xlabel=f\"Loadings, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Loadings, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "# Get the 10 largest loadings along PC1 for highlighting:\n",
    "highlighted_indices = np.argsort(abs(pc1_loadings))[-10:]\n",
    "print(f\"10 largest along PC1: {highlighted_indices}\")\n",
    "\n",
    "for i, (xi, yi) in enumerate(zip(pc1_loadings, pc2_loadings)):\n",
    "    if i in highlighted_indices:\n",
    "        txt = ax.text(xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\")\n",
    "        txt.set_path_effects(\n",
    "            [\n",
    "                pe.withStroke(linewidth=1.5, foreground=\"yellow\"),\n",
    "                pe.Normal(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        txt = ax.text(\n",
    "            xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\", color=\"0.7\"\n",
    "        )\n",
    "\n",
    "\n",
    "ax.set_xlim(-0.15, 0.15)\n",
    "ax.set_ylim(-0.15, 0.15)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7beaa8",
   "metadata": {},
   "source": [
    "Finally, we create a scatterplot to show how two genes with high loadings (for PC1) separate the tissue samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene1 = 1490\n",
    "gene2 = 92\n",
    "grid = sns.jointplot(\n",
    "    data=data_ovo,\n",
    "    x=f\"X.{gene1 + 1}\",\n",
    "    y=f\"X.{gene2 + 1}\",\n",
    "    hue=\"class\",\n",
    ")\n",
    "ax = grid.fig.axes[0]\n",
    "ax.set_xlabel(f\"Gene expression for {gene1}\")\n",
    "ax.set_ylabel(f\"Gene expression for {gene2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c552259",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(a): Did you find any promising genes?\n",
    "Yes, genes with the highest absolute loadings on Principal Component 1 (PC1) appear promising for distinguishing between the samples. For example 1490 and 92 as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea56348",
   "metadata": {},
   "source": [
    "### 6.2(b)\n",
    "\n",
    "**Task: In the following task, you will develop a classifier to predict whether a tissue sample is cancerous or non-cancerous based on gene expression data. Which error type (false positive or false negative) should be minimised?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9cf63",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(b): Will you minimise false positives or negatives?\n",
    "\n",
    "While a false positive could result in unnecessary and potentially invasive medical procedures, a false negative may be a greater risk by delaying treatment. Assuming that starting medical treatment is not only based on our classifier and that secondary testing will be used, then a false positive made by our classifier would be identified in subsequent tests. Therefore, to minimise the risk of overlooking cancerous samples and delaying treatment, we prioritise the minimisation of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a9963",
   "metadata": {},
   "source": [
    "### 6.2(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify tissue type from the gene expressions. Optimize the tree depth using cross-validation on a training set. Report the optimal maximum depth of the resulting tree.**\n",
    "\n",
    "With reference to the previous problem:\n",
    "\n",
    "* If you prioritised minimising false positives, use the `precision` as your optimisation metric.\n",
    "* If you prioritised minimising false negatives, use the `recall` as your optimisation metric.\n",
    "* If you opted for a balanced approach, use the `balanced_accuracy` as your optimisation metric.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the decision tree can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c71485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ovo, y_ovo, stratify=classes, test_size=0.33, random_state=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d72be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Use recall to minimize false negatives\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b87d1",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(c): What depth did you get for your tree?\n",
    "The optimal depth was 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc51e4",
   "metadata": {},
   "source": [
    "### 6.2(d)\n",
    "\n",
    "**Task: Create a k-nearest neighbours classifier to classify tissue type from the gene expressions. Optimize the number of neighbours using cross-validation on a training set. Report the optimal number of neighbours.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the k-nearest neighbours classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 15)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3886e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 15)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # To minimize false negatives\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a116b6",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(d): What was the optimal number of neighbours?\n",
    "The optimal number of neighbours was 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a61c",
   "metadata": {},
   "source": [
    "### 6.2(e)\n",
    "\n",
    "**Task: Create a random forest classifier to classify tissue type from the gene expressions. Optimize the number of trees and levels using cross-validation on a training set. Report the optimal number of trees and levels.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the random forest classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 11),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_forest = grid.best_estimator_\n",
    "print(\"Best forest:\", best_forest)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7828d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 10),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=2025),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_forest = grid.best_estimator_\n",
    "print(\"Best forest:\", best_forest)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9512269",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(e): What was the optimal number of estimators and tree depth?\n",
    "The optimal number of estimators was 50 and the depth was 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894199b8",
   "metadata": {},
   "source": [
    "### 6.2(f)\n",
    "\n",
    "**Task: Compare the three optimised classifiers you have made by applying them to the test set and obtaining the corresponding confusion matrices. Also compute the [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), and the [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) for the test set. Which classifier performs best?**\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The metrics can be computed as follows:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "y_hat = best_tree.predict(X_test)\n",
    "recall_tree = recall_score(y_test, y_hat)\n",
    "precision_tree = precision_score(y_test, y_hat)\n",
    "bac_tree = balanced_accuracy_score(y_test, y_hat)\n",
    "print(f\"Recall: {recall_tree:.3f}\")\n",
    "print(f\"Precision: {precision_tree:.3f}\")\n",
    "print(f\"Balanced accuracy: {bac_tree:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9, 3))\n",
    "\n",
    "axes[0].set_title(\"Decision tree (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=False,\n",
    "    ax=axes[0],\n",
    ")\n",
    "fig.colorbar(axes[0].images[0], ax=axes[0], shrink=0.7)\n",
    "\n",
    "axes[1].set_title(\"kNN (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_knn, X_test, y_test, colorbar=False, ax=axes[1]\n",
    ")\n",
    "fig.colorbar(axes[1].images[0], ax=axes[1], shrink=0.7)\n",
    "\n",
    "axes[2].set_title(\"Random forest (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_forest, X_test, y_test, colorbar=False, ax=axes[2]\n",
    ")\n",
    "fig.colorbar(axes[2].images[0], ax=axes[2], shrink=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83670868",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"Classifier\": [],\n",
    "    \"Recall (test)\": [],\n",
    "    \"Precision (test)\": [],\n",
    "    \"Balanced accuracy (test)\": [],\n",
    "}\n",
    "for cls in (best_tree, best_knn, best_forest):\n",
    "    name = str(cls)\n",
    "    print(name)\n",
    "    y_hat = cls.predict(X_test)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    precision = precision_score(y_test, y_hat)\n",
    "    bac = balanced_accuracy_score(y_test, y_hat)\n",
    "    table[\"Classifier\"].append(name)\n",
    "    table[\"Recall (test)\"].append(recall)\n",
    "    table[\"Precision (test)\"].append(precision)\n",
    "    table[\"Balanced accuracy (test)\"].append(bac)\n",
    "    print(f\"\\t-Recall: {recall:.3f}\")\n",
    "    print(f\"\\t-Precision: {precision:.3f}\")\n",
    "    print(f\"\\t-Balanced accuracy: {bac:.3f}\")\n",
    "table = pd.DataFrame(table)\n",
    "table.sort_values(by=\"Recall (test)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace6af5",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(f): Which classifier performs best?\n",
    "From these results, we see that the random forest classifier makes fewer false negative mistakes (resulting in a higher recall). However, it produces one additional false positive compared to the k-nearest neighbours classifier (resulting in a lower precision). Since we focus on minimizing the number of false negatives (as per our answer to 6.2(b)), the random forest classifier performs best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
